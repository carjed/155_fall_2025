# Multiple logistic regression




```{r include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, 
  warning = FALSE,
  message = FALSE,
  error = TRUE)
```


<!-- create student notes from activity in core-->

{{< include _create_student_notes.qmd >}}

```{r}
#| echo: false
#| eval: true
make_student_notes('Multiple logistic regression', '_L17-logistic-multiple-evaluation.qmd')
```

<!-- pull activity from core -->

```{r knitr-opts}
#| echo: false
#| cache: false
knitr::opts_chunk$set(eval = FALSE, echo = TRUE)
```

{{< include core/_L17-logistic-multiple-evaluation.qmd >}}

\
\
\
\


# Solutions

```{r eval = TRUE, echo = FALSE}
library(readr)
library(ggplot2)
library(ggmosaic)
library(dplyr)
library(broom)

resume <- read_csv("https://mac-stat.github.io/data/resume.csv")
```


## Exercise 1: Graphical and numerical summaries

```{r eval = TRUE}
ggplot(resume) + 
    geom_mosaic(aes(x = product(gender), fill = received_callback)) +
    scale_fill_manual("Received Callback? \n(1 = yes, 0 = no)", values = c("lightblue", "steelblue")) + 
    labs(x = "Inferred Binary Gender (f = female, m = male)", y = "Received Callback? (1 = yes, 0 = no)")

ggplot(resume) + 
    geom_mosaic(aes(x = product(gender), fill = received_callback)) +
    facet_grid(. ~ race) +
    scale_fill_manual("Received Callback? \n(1 = yes, 0 = no)", values = c("lightblue", "steelblue")) + 
    labs(x = "Inferred Binary Gender (f = female, m = male)", y = "Received Callback? (1 = yes, 0 = no)")

ggplot(resume) + 
    geom_mosaic(aes(x = product(received_callback, race), fill = received_callback)) +
    facet_grid(. ~ gender) +
    scale_fill_manual("Received Callback? \n(1 = yes, 0 = no)", values = c("lightblue", "steelblue")) + 
    labs(x = "Inferred Race", y = "Received Callback? (1 = yes, 0 = no)")

resume %>%
    group_by(race, gender) %>%
    count(received_callback) %>%
    group_by(race, gender) %>%
    mutate(condprop = n/sum(n))
```

Overall, a small proportion of applicants received a callback, with those who were inferred to be black males being least likely to get a callback (5.8%) and those inferred to be white females being most likely to get a callback (9.9%). In general, job applicants whose race was inferred to be white were more likely to receive a callback than those whose race was inferred to be black, regardless of their inferred gender. On the other hand, inferred gender does not seem to have as much of an effect on the chance of receiving a callback, with perhaps just a slight advantage for females. 



## Exercise 2: Logistic regression modeling

```{r eval = TRUE}
mod1 <- glm(received_callback ~ gender + race, data = resume, family = "binomial")

tidy(mod1) %>%
    select(term, estimate) %>%
    mutate(estimate_exp = exp(estimate))
```

- exp(Intercept): We estimate the odds of getting a callback among those inferred to be black females is only 0.07, meaning that the chance of getting a callback is 0.07 times as large as the chance of not getting a callback (or, inversely, the chance of *not* getting a callback is 1/0.07 = `r round(1/0.07,2)`) times greater than the chance of getting a callback). 
- exp(genderm): Comparing applicants of the same inferred race, we estimate that those inferred to be male have an odds of getting a callback that is 0.88 times as high as (or, equivalently, 12% lower than) the odds of getting a callback for those inferred to be female.
- exp(racewhite): We estimate that the odds of getting a callback are 1.55 times higher for applicants whose race was inferred to be white as compared to those who were inferred to be black but the same gender.



## Exercise 3: Interaction terms

a. Including an interaction term in our model would allow us to investigate whether the effect of race on getting a callback depends on your gender or, vice versa, if the effect of gender on getting a callback depends on a race. In other words, we could ask questions like: is there more of a discrepancy in callbacks between black and white males than there is among black and white females? Is there more of a discrepancy in callbacks between male and female blacks than there is among male and female whites?

b. Let's try adding an interaction between gender and race. Update the code below to fit this new interaction model.

```{r eval = TRUE}
mod2 <- glm(received_callback ~ gender * race, data = resume, family = "binomial")

tidy(mod2) %>%
    select(term, estimate) %>%
    mutate(estimate_exp = exp(estimate))
```

c. Overall logistic regression model formula in terms of beta coefficients:

$$\log(Odds[ReceivedCallback = 1 \mid gender, race]) = \beta_0 + \beta_1 genderm + \beta_2 racewhite + \beta_3 genderm \times racewhite$$

The model formula for males:

$$\log(Odds[ReceivedCallback = 1 \mid gender=m, race]) = (\beta_0 + \beta_1) + (\beta_2 + \beta_3) racewhite$$

The model formula for females:

$$\log(Odds[ReceivedCallback = 1 \mid gender=f, race]) = \beta_0 + \beta_2 racewhite$$
Focusing first on the female model formula, we can see that this is a simple logistic regression model. 

- exp(beta0): Odds of callback for black females
- exp(beta2): This is the odds ratio for race among females. That is, white females have exp(beta2) times the odds of callback than black females.

Then focusing on the male model formula, we can see that this is also a simple logistic regression model.

- exp(beta0+beta1): Odds of callback for black males
- exp(beta2+beta3): This is the odds ratio for race among males. White males have exp(beta2+beta3) times the odds of callback than black males.

Comparing the male to the female model formula, we have:

- exp(beta1): Black males have exp(beta1) times the odds of a callback than black females
- exp(beta3): This tells us how many times higher the odds ratio for race is in males as compared to females.


## Exercise 4: Prediction

```{r eval = TRUE}
# set up data frame with people we want to predict for
predict_data <- data.frame(
    gender = c("f", "m", "f", "m"),
    race = c("black", "black", "white", "white")
)
print(predict_data)

# prediction based on model without interaction
mod1 %>%
    predict(newdata = predict_data, type = "response")

# prediction based on model with interaction
mod2 %>%
    predict(newdata = predict_data, type = "response")
```

The predicted probabilities from our logistic regression models show that we estimate those inferred to be black males have the lowest chance of receiving a callback (5.87% based on `mod1` and 5.83% based on `mod2`), followed by black females (6.62% and 6.63%), white males (8.83% and 8.87%), and then white females (9.91% and 9.89%). This matches the trend we observed that those inferred to be white have a greater chance of getting a callback, regardless of gender, and that those who are inferred to be female have a slightly higher chance of getting a callback than those inferred to be male. 



## Exercise 5: Evaluating logistic models with plots

```{r eval = TRUE}
mod3 <- glm(received_callback ~ gender*race + years_college + years_experience + resume_quality, data = resume, family = "binomial")

mod1_output <- augment(mod1, type.predict = "response")
mod2_output <- augment(mod2, type.predict = "response")
mod3_output <- augment(mod3, type.predict = "response")

ggplot(mod1_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot()
ggplot(mod2_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot()
ggplot(mod3_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot()
```

a. All 3 models show that those who actually received a callback had higher predicted probabilities of a callback. Models 1 and 2 are very similar--although predicted probabilites of callback are high for those who did actually receive a callback, there is substantial overlap in the boxplots. There is more separation between the boxplots in the third model, perhaps model 3 is best in terms of accuracy.

b. We would want to place the vertical lines such that as much of the left boxplot was below the line (low predicted probabilities for those with Y = 0) and as much of the right boxplot was above the line (high predicted probabilities for those with Y = 1).



## Exercise 6: Evaluating logistic models with evaluation metrics

```{r eval = TRUE}
ggplot(mod1_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot() +
    geom_hline(yintercept = 0.08, color = "red")
ggplot(mod2_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot() +
    geom_hline(yintercept = 0.08, color = "red")
ggplot(mod3_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot() +
    geom_hline(yintercept = 0.08, color = "red")
```


Next, we can use our threshold to classify each person in our dataset based on their predicted probability of getting a callback: we'll predict that everyone with a predicted probability higher than our threshold got a callback, and otherwise they did not. Then, we'll compare our model's prediction to the true outcome (whether or not they actually did get a callback).

```{r eval = TRUE}
threshold <- 0.08
mod1_output %>%
    mutate(predictCallback = .fitted >= threshold) %>% 
    count(received_callback, predictCallback) 

mod2_output %>%
    mutate(predictCallback = .fitted >= threshold) %>%
    count(received_callback, predictCallback)

mod3_output %>%
    mutate(predictCallback = .fitted >= threshold) %>%
    count(received_callback, predictCallback)
```

a. 

> Models 1 and 2: (Both models result in the same confusion matrix.)

                        Predict callback   Predict no callback    Total
---------------------- ------------------ ---------------------- -------
Actually got callback       235               157                  392
Actually did not           2200              2278                 4478
Total                      2435              2435                 4870

> Model 3: 

                        Predict callback   Predict no callback    Total
---------------------- ------------------ ---------------------- -------
Actually got callback      233               159                  392
Actually did not           2013              2465                 4478
Total                      2246              2624                 4870

b. Now compute the following evaluation metrics for the models:

> Models 1 and 2:
> 
> - Accuracy: P(Predict Y Correctly) = (235 + 2278)/(235 + 157 + 2200 + 2278) = `r (235 + 2278)/(235 + 157 + 2200 + 2278)`
> - Sensitivity: P(Predict Y = 1 | Actual Y = 1) = 235/(235 + 157) = `r 235/(235 + 157)`
> - Specificity: P(Predict Y = 0 | Actual Y = 0) = 2278/(2200 + 2278) = `r 2278/(2200 + 2278)`
> - False negative rate: P(Predict Y = 0 | Actual Y = 1) = 157/(235 + 157) = `r 157/(235 + 157)` (notice that this is equal to 1 - Sensitivity)
> - False positive rate: P(Predict Y = 1 | Actual Y = 0) = 2200/(2200 + 2278) = `r 2200/(2200 + 2278)` (notice that this is equal to 1 - Specificity)

> Model 3: 
> 
> - Accuracy: P(Predict Y Correctly) = (233 + 2465)/(233 + 159 + 2013 + 2465) = `r (233 + 2465)/(233 + 159 + 2013 + 2465)`
> - Sensitivity: P(Predict Y = 1 | Actual Y = 1) = 233/(233 + 159) = `r 233/(233 + 159)`
> - Specificity: P(Predict Y = 0 | Actual Y = 0) = 2465/(2013 + 2465) = `r 2465/(2013 + 2465)`
> - False negative rate: P(Predict Y = 0 | Actual Y = 1) = 159/(233 + 159) = `r 159/(233 + 159)` (notice that this is equal to 1 - Sensitivity)
> - False positive rate: P(Predict Y = 1 | Actual Y = 0) = 2013/(2013 + 2465) = `r 2013/(2013 + 2465)` (notice that this is equal to 1 - Specificity)

c. Imagine that we are a career center on a college campus and we want to use this model to help students that are looking for jobs. Consider the consequences of incorrectly predicting whether or not an individual will get a callback. What are the consequences of a false negative? What about a false positive? Which one is worse?
    - False Negatives (predicting no callback, but actually got callback): this would be a lost opportunity if a student decided not to submit their resume, thinking they wouldn't get a callback, when actually they would have. 
    - False Positives (predicting callback, but actually didn't get callback): this would be a disappointment for the student, thinking they were going to get a callback but they ended up not getting one.


