```{r setup}
#| include: false
knitr::opts_chunk$set(
  collapse = TRUE, 
  warning = FALSE,
  message = FALSE,
  error = TRUE,
  fig.height = 2.75, 
  fig.width = 4.25,
  fig.env = 'figure',
  fig.pos = 'h',
  fig.align = 'center')
```

# Annoucements, etc.

-   [**Put away cell phones (like in your backpack, not face down on your table).**]{.underline}

-   Download [17-logistic-multiple-evaluation-notes.qmd](template_qmds/17-logistic-multiple-evaluation-notes.qmd), open it in RStudio, and save it in the ‚ÄúActivities‚Äù sub-folder of the ‚ÄúSTAT 155‚Äù folder.

### Upcoming dates

**This Week:**

-   Wednesday: midterm grades due

-   Thursday: PP5 due at 11:59pm

------------------------------------------------------------------------

**On the horizon:**

-   Checkpoint 13 due before class on **Tuesday, October 28**
-   Quiz 2 is on **Thursday,** **October 30**
    -   Covering all material up through multiple logistic regression
    -   Preceptor review session tentatively on **Sunday, October 26**

# Notes

## Learning goals

By the end of this lesson, you should be able to:

-   Construct multiple logistic regression models in R
-   Interpret coefficients in multiple logistic regression models
-   Use multiple logistic regression models to make predictions
-   Evaluate the quality of logistic regression models by using predicted probability boxplots and by computing and interpreting accuracy, sensitivity, specificity, false positive rate, and false negative rate

## Readings and videos

Please go through the following reading or videos **before** class.

-   Reading: Section 4.4 in the [STAT 155 Notes](https://mac-stat.github.io/Stat155Notes/)

-   Videos:

    -   [Part 1: Concepts](https://youtu.be/jisiKHHDz0U) ([script](https://drive.google.com/file/d/1ohEyz_ZLce8KW8LE8hftWYZVR--mVb0A/view?usp=sharing))
    -   [Part 2: R Code](https://youtu.be/1iF0ZaUBGT4) ([script](https://drive.google.com/file/d/1cuJZcgZhEvESeApRA6dR36wUeQfQ0En9/view?usp=sharing))

**File organization:** Save this file in the "Activities" subfolder of your "STAT155" folder.

# Warm-up

## Exercise 1

It's been a while! Today we'll start with a dataset of Hot Ones episodes to really warm up üî•. Run the code chunk below to load and merge the data:

```{r}
library(tidyverse)

episodes <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-08/episodes.csv')
sauces <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-08/sauces.csv')

sauces_summary <- sauces %>% 
  mutate(scoville10k=scoville/10000) %>%
  group_by(season) %>% 
  summarise(median_scoville = median(scoville10k), 
            max_scoville=max(scoville10k), 
            total_scoville=sum(scoville10k), 
            var_scoville=var(scoville10k))

# glance at the data
head(sauces_summary)

episodes_full <- episodes %>% left_join(sauces_summary)
head(episodes_full)

```

## Exercise 2

We are primarily interested in developing a model to predict whether a contestant `finished` the challenge. `finished` is a binary variable (TRUE/FALSE), so we want to use a logistic regression model. Let's start by looking at the `median_scoville` variable as a predictor (i.e., among the 10 sauces in the challenge for a given season, what is the median intensity in scoville units of the hot sauces?).

Work with your group to see if you remember the syntax for fitting a logistic regression model and pull up the model summary:

```{r}

```

## Exercise 3

Interpret the intercept and `median_scoville` coefficients for the model you fit above. Your interpretations should be on the *odds* scale (remember that in logistic regression, we are modeling the *log-odds* of finishing the challenge! Also remember that we've rescaled the scoville units, so a "1-unit increase in median_scoville" corresponds to an increase of 10,000 scoville units):

> Response

## Exercise 4

Suppose the producers are planning a new season and want to reduce the number of guests who complete the challenge. Assuming our model is reasonably strong, what is the median number of scoville units they should aim for to minimize the odds of finishing? (this is not meant to be a trick question but it might feel like it is)

> Response

## Exercise 5

Take a look at some of the other variables in the `episodes_full` data frame. What are some other predictors you think might be useful to try? Fit at least two other simple logistic regression models below and look at the model summary output:

```{r}

```

## Exercise 5b [optional, depending on time]

If we use `guest_appearance_number` as a predictor, what are the predicted odds of finishing for a contestant on their third appearance? What is the predicted *probability* of that guest finishing?

## Exercise 6

How can we evaluate which model is "best"? (note that we do not have multiple or adjusted R^2 for logistic regression models).

> Response

Today we'll explore both how to expand logistic regression into multiple logistic regression, interpret coefficients, use logistic regression models for prediction, and evaluate their performance.


# Exercises

**Context:** In this activity, we'll look at data from an experiment conducted in 2001-2002 that investigated the influence of race and gender on job applications. The researchers created realistic-looking resumes and then randomly assigned a name to the resume that "would communicate the applicant's gender and race" (e.g., they they assumed the name Emily would generally be interpreted as a white woman, whereas the name Jamal would generally be interpreted as a black man). They then submitted these resumes to job postings in Boston and Chicago and waited to see if the applicant got a call back from the job posting.

You can find a full description of the variables in this dataset [here](https://www.openintro.org/data/index.php?data=resume). Today, we'll focus on the following variables:

-   `received_callback`: indicator that the resume got a call back from the job posting
-   `gender`: inferred binary gender associated with the first name on the resume
-   `race`: inferred race associated with the first name on the resume

Our research question is: **does an applicant's inferred gender and race have an effect on the chance that they receive a callback after submitting their resume for an open job posting?**

You'll need to run `install.packages("broom")` in the Console first.

```{r}
library(readr)
library(ggplot2)
library(ggmosaic)
library(dplyr)
library(broom)

resume <- read_csv("https://mac-stat.github.io/data/resume.csv")
```

## Exercise 1: Graphical and numerical summaries

Our research question involves three categorical variables: `received_callback` (1 = yes, 0 = no), `gender` (f = female, m = male), and `race` (Black, White). Let's start by creating a mosaic plot to visually compare inferred binary gender and callbacks:

```{r graphical-summary-two-categorical-variables}
# create mosaic plot of callback vs gender
ggplot(resume) + 
    geom_mosaic(aes(x = product(gender), fill = received_callback)) +
    scale_fill_manual("Received Callback? \n(1 = yes, 0 = no)", values = c("lightblue", "steelblue")) + 
    labs(x = "Inferred Binary Gender (f = female, m = male)", y = "Received Callback? (1 = yes, 0 = no)")
```

In this activity, we're also interested in looking at the relationship between inferred race and callbacks. One way we can add a third variable to a plot is to use the `facet_grid` function, particularly when that third variable is categorical. Let's try that now:

```{r graphical-summary-three-variables}
# create mosaic plot of callback vs gender and race
ggplot(resume) + 
    geom_mosaic(aes(x = product(gender), fill = received_callback)) +
    facet_grid(. ~ race) +
    scale_fill_manual("Received Callback? \n(1 = yes, 0 = no)", values = c("lightblue", "steelblue")) + 
    labs(x = "Inferred Binary Gender (f = female, m = male)", y = "Received Callback? (1 = yes, 0 = no)")
```

Here's another way of looking at the relationship between these three variables, switching the placement of `gender` and `race` in the mosaic plot:

```{r graphical-summary-three-variables-version2}
# create mosaic plot of callback vs gender and race
ggplot(resume) + 
    geom_mosaic(aes(x = product(received_callback, race), fill = received_callback)) +
    facet_grid(. ~ gender) +
    scale_fill_manual("Received Callback? \n(1 = yes, 0 = no)", values = c("lightblue", "steelblue")) + 
    labs(x = "Inferred Race", y = "Received Callback? (1 = yes, 0 = no)")
```

When we are comparing three categorical variables, a useful numerical summary is to calculate relative frequencies/proportions of cases falling into each category of the outcome variable, conditional on which categories of the explanatory variables they fall into. Run this code chunk to calculate the conditional proportion of resumes that did nor did not receive a callback, given the inferred gender and race of the applicant:

```{r numerical-summaries}
# corresponding numerical summaries
resume %>%
    group_by(race, gender) %>%
    count(received_callback) %>%
    group_by(race, gender) %>%
    mutate(condprop = n/sum(n))
```

Write a short description that summarizes the information you gain from these visualizations and numerical summaries. Write this summary using good sentences that tell a story and do not resemble a checklist. Don't forget to consider the context of the data, and make sure that your summary addresses our research question: *does an applicant's inferred gender or race have an effect on the chance that they receive a callback?*

## Exercise 2: Logistic regression modeling

Next, we'll fit a logistic regression model to these data, modeling the *log odds* of receiving a callback as a function of the applicant's inferred gender and race:

$$\log(Odds[ReceivedCallback = 1 \mid gender, race]) = \beta_0 + \beta_1 genderm + \beta_2 racewhite$$

Fill in the blanks in the code below to fit this logistic regression model.

```{r logistic-model}
# fit logistic model and save it as object called "mod1"
mod1 <- glm(received_callback ~ gender + race, data = ___, family = ___)
```

Then, run the code chunk below to get the coefficient estimates and exponentiated estimates, presented in a nicely formatted table:

```{r get-coefficient-estimates-mod1}
# print out tidy summary of mod, focusing on estimates & exponentiated estimates
tidy(mod1) %>%
    select(term, estimate) %>%
    mutate(estimate_exp = exp(estimate))
```

Write an interpretation of each of the exponentiated coefficients in your logistic regression model.

## Exercise 3: Interaction terms

a.  Do you think it would make sense to add an interaction term (between gender and race) to our logistic regression model? Why/why not?

b.  Let's try adding an interaction between gender and race. Update the code below to fit this new interaction model.

```{r logistic-model-with-interaction}
# fit logistic model and save it as object called "mod2"
mod2 <- glm(received_callback ~ ___, data = resume, family = ___)
```

Then, run the code chunk below to get the coefficient estimates and exponentiated estimates for this interaction model, presented in a nicely formatted table:

```{r get-coefficient-estimates-mod2}
# print out tidy summary of mod, focusing on estimates & exponentiated estimates
tidy(mod2) %>%
    select(term, estimate) %>%
    mutate(estimate_exp = exp(estimate))
```

c.  (CHALLENGE) Write out the logistic regression model formula separately for males and for females. Based on this how would we interpret the exponentiated coefficients in this model?

## Exercise 4: Prediction

We can use our models to predict whether or not a resume will receive a call back based on the inferred gender and race of the applicant. Run the code below to use the `predict()` function to predict the probability of getting a call back for four job applicants: a person inferred to be a black female, a person inferred to be black male, a person inferred to be a white female, and a person inferred to be a white male.

```{r predict}
# set up data frame with people we want to predict for
predict_data <- data.frame(
    gender = c("f", "m", "f", "m"),
    race = c("black", "black", "white", "white")
)
print(predict_data)

# prediction based on model without interaction
mod1 %>%
    predict(newdata = predict_data, type = "response")

# prediction based on model with interaction
mod2 %>%
    predict(newdata = predict_data, type = "response")
```

Report and compare the predictions we get from `predict()`. Do they make sense to you based on your understanding of the data? Combine insights from visualizations and modeling to write a few sentences summarizing findings for our research question: *does an applicant's inferred gender and race have an effect on the chance that they receive a callback after submitting their resume for an open job posting?*

## Exercise 5: Evaluating logistic models with plots

We'll fit one more model that adds on to the interaction model to also include years of college, years of work experience, and resume quality. The `augment()` code takes our fitted models and stores the predicted probabilities in a variable called `.fitted`. Then we use boxplots to show the predicted probabilities of receiving a callback in those who actually did and did not receive a callback.

```{r}
mod3 <- glm(received_callback ~ gender*race + years_college + years_experience + resume_quality, data = resume, family = "binomial")

mod1_output <- augment(mod1, type.predict = "response") # Store predicted probabilities in a variable called .fitted
mod2_output <- augment(mod2, type.predict = "response")
mod3_output <- augment(mod3, type.predict = "response")

ggplot(mod1_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot()
ggplot(mod2_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot()
ggplot(mod3_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot()
```

a.  Summarize what you learn about the ability of the 3 models to differentiate those who actually did and did not receive a callback. What model seems best, and why?

b.  If you had to draw a horizontal line across each of the boxplots that vertically separates the left and right boxplots well, where would you place them?

## Exercise 6: Evaluating logistic models with evaluation metrics

Sometimes we may need to go beyond the predicted probabilities from our model and try to classify individuals into one of the two binary outcomes (received or did not receive a callback). How high of a predicted probability would we need from our model in order to be convinced that the person actually got a callback? This is the idea behind the horizontal lines that we drew in the previous exercise.

Let's explore using a probability threshold of 0.08 (8%) to make a binary prediction for each case:

-   If a model's predicted probability of getting a callback is greater than or equal to 8.5%, we'll predict they got a callback.
-   If the predicted probability is below 8%, we'll predict they didn't get a callback.

We can visualize this threshold on our predicted probability boxplots:

```{r}
ggplot(mod1_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot() +
    geom_hline(yintercept = 0.08, color = "red")
ggplot(mod2_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot() +
    geom_hline(yintercept = 0.08, color = "red")
ggplot(mod3_output, aes(x = factor(received_callback), y = .fitted)) +
    geom_boxplot() +
    geom_hline(yintercept = 0.08, color = "red")
```

Next, we can use our threshold to classify each person in our dataset based on their predicted probability of getting a callback: we'll predict that everyone with a predicted probability higher than our threshold got a callback, and otherwise they did not. Then, we'll compare our model's prediction to the true outcome (whether or not they actually did get a callback).

```{r}
# get binary predictions for mod1 and compare to truth
threshold <- 0.08
mod1_output %>%
    mutate(predictCallback = .fitted >= threshold) %>% ## predict callback if probability greater than or equal to threshold
    count(received_callback, predictCallback) ## compare actual and predicted callbacks

mod2_output %>%
    mutate(predictCallback = .fitted >= threshold) %>%
    count(received_callback, predictCallback)

mod3_output %>%
    mutate(predictCallback = .fitted >= threshold) %>%
    count(received_callback, predictCallback)
```

We can use the `count()` output to fill create contingency tables of the results. (These tables are also called **confusion matrices**.)

a.  Fill in the confusion matrix for Model 3.

> Models 1 and 2: (Both models result in the same confusion matrix.)

|                       | Predict callback | Predict no callback | Total |
|-----------------------|:----------------:|:-------------------:|:-----:|
| Actually got callback |       235        |         157         |  392  |
| Actually did not      |       2200       |        2278         | 4478  |
| Total                 |       2435       |        2435         | 4870  |

> Model 3:

|                       | Predict callback | Predict no callback |  Total   |
|-----------------------|:----------------:|:-------------------:|:--------:|
| Actually got callback |     \_\_\_\_     |      \_\_\_\_       | \_\_\_\_ |
| Actually did not      |     \_\_\_\_     |      \_\_\_\_       | \_\_\_\_ |
| Total                 |     \_\_\_\_     |      \_\_\_\_       | \_\_\_\_ |

b.  Now compute the following evaluation metrics for the models:

Models 1 and 2:

-   Accuracy: P(Predict Y Correctly)
-   Sensitivity: P(Predict Y = 1 \| Actual Y = 1)
-   Specificity: P(Predict Y = 0 \| Actual Y = 0)
-   False negative rate: P(Predict Y = 0 \| Actual Y = 1)
-   False positive rate: P(Predict Y = 1 \| Actual Y = 0)

Model 3:

-   Accuracy: P(Predict Y Correctly)
-   Sensitivity: P(Predict Y = 1 \| Actual Y = 1)
-   Specificity: P(Predict Y = 0 \| Actual Y = 0)
-   False negative rate: P(Predict Y = 0 \| Actual Y = 1)
-   False positive rate: P(Predict Y = 1 \| Actual Y = 0)

c.  Imagine that we are a career center on a college campus and we want to use this model to help students that are looking for jobs. Consider the consequences of incorrectly predicting whether or not an individual will get a callback. What are the consequences of a false negative? What about a false positive? Which one is worse?

## Reflection

What are some similarities and differences between how we interpret and evaluate linear and logistic regression models?

> **Response:** Put your response here.
